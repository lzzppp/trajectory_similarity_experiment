{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Bypass(nn.Module):\n",
    "    _supported_styles = ['residual', 'highway']\n",
    "    \n",
    "    @classmethod\n",
    "    def supports_style(cls, style):\n",
    "        return style.lower() in cls._supported_styles\n",
    "\n",
    "    def __init__(self, style, residual_scale=True, highway_bias=-2, input_size=None):\n",
    "        super(Bypass, self).__init__()\n",
    "        assert self.supports_style(style)\n",
    "        self.style = style.lower()\n",
    "        self.residual_scale = residual_scale\n",
    "        self.highway_bias = highway_bias\n",
    "        self.highway_gate = nn.Linear(input_size[1], input_size[0])\n",
    "\n",
    "    def forward(self, transformed, raw):\n",
    "        assert transformed.shape[:-1] == raw.shape[:-1]\n",
    "\n",
    "        tsize = transformed.shape[-1]\n",
    "        rsize = raw.shape[-1]\n",
    "        adjusted_raw = raw\n",
    "        if tsize < rsize:\n",
    "            assert rsize / tsize <= 50\n",
    "            if rsize % tsize != 0:\n",
    "                padded = F.pad(raw, (0, tsize - rsize % tsize))\n",
    "            else:\n",
    "                padded = raw\n",
    "            adjusted_raw = padded.view(*raw.shape[:-1], -1, tsize).sum(-2) * math.sqrt(\n",
    "                tsize / rsize)\n",
    "        elif tsize > rsize:\n",
    "            multiples = math.ceil(tsize / rsize)\n",
    "            adjusted_raw = raw.repeat(*([1] * (raw.dim() - 1)), multiples).narrow(\n",
    "                -1, 0, tsize)\n",
    "\n",
    "        if self.style == 'residual':\n",
    "            res = transformed + adjusted_raw\n",
    "            if self.residual_scale:\n",
    "                res *= math.sqrt(0.5)\n",
    "            return res\n",
    "        elif self.style == 'highway':\n",
    "            transform_gate = torch.sigmoid(self.highway_gate(raw) + self.highway_bias)\n",
    "            carry_gate = 1 - transform_gate\n",
    "            return transform_gate * transformed + carry_gate * adjusted_raw\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    _supported_nonlinearities = [\n",
    "        'sigmoid', 'tanh', 'relu', 'elu', 'selu', 'glu', 'leaky_relu'\n",
    "    ]\n",
    "\n",
    "    @classmethod\n",
    "    def supports_nonlinearity(cls, nonlin):\n",
    "        return nonlin.lower() in cls._supported_nonlinearities\n",
    "\n",
    "    def __init__(self,\n",
    "              style,\n",
    "              layers=1,\n",
    "              bypass_network=None,\n",
    "              non_linearity='leaky_relu',\n",
    "              hidden_size=None,\n",
    "              output_size=None,\n",
    "              input_size=None):\n",
    "        super(Transform, self).__init__()\n",
    "        hidden_size = hidden_size or input_size\n",
    "        output_size = output_size or hidden_size\n",
    "\n",
    "        parts = style.split('-')\n",
    "\n",
    "        if 'layer' in parts:\n",
    "            layers = int(parts[parts.index('layer') - 1])\n",
    "        \n",
    "        for part in parts:\n",
    "            if Bypass.supports_style(part):\n",
    "                bypass_network = part\n",
    "            if Transform.supports_nonlinearity(part):\n",
    "                non_linearity = part\n",
    "\n",
    "        self.transforms = nn.ModuleList()\n",
    "        self.bypass_networks = nn.ModuleList()\n",
    "\n",
    "        assert (non_linearity is None or self.supports_nonlinearity(non_linearity))\n",
    "        self.non_linearity = non_linearity.lower() if non_linearity else None\n",
    "\n",
    "        transform_in_size = input_size\n",
    "        transform_out_size = hidden_size\n",
    "\n",
    "        for layer in range(layers):\n",
    "            if layer == layers - 1:\n",
    "                transform_out_size = output_size\n",
    "            self.transforms.append(nn.Linear(transform_in_size, transform_out_size))\n",
    "            self.bypass_networks.append(Bypass(\"highway\", input_size=[hidden_size, hidden_size]))\n",
    "            transform_in_size = transform_out_size\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = input_data\n",
    "\n",
    "        for transform, bypass in zip(self.transforms, self.bypass_networks):\n",
    "            new_output = transform(output)\n",
    "            if self.non_linearity:\n",
    "                new_output = getattr(F, self.non_linearity)(new_output)\n",
    "            if bypass:\n",
    "                new_output = bypass(new_output, output)\n",
    "            output = new_output\n",
    "\n",
    "        return output\n",
    "class RNNEncoder (nn.Module):\n",
    "    def __init__(self):\n",
    "        super (RNNEncoder, self).__init__ ()\n",
    "        self.rnn_encoder = nn.LSTM (input_size=130, hidden_size=128, batch_first=True)\n",
    "        self.memory = nn.Embedding (14000, 128, padding_idx=0)\n",
    "        self.transform = Transform(\"2-layer-highway\", input_size=512, hidden_size=512, output_size=512)\n",
    "    \n",
    "    def forward(self, gps_data1, grid_data1, gps_data2, grid_data2):\n",
    "#         grid_data1 = self.memory (grid1)\n",
    "#         grid_data2 = self.memory (grid2)\n",
    "        \n",
    "        input_data1 = torch.cat ((gps_data1, grid_data1), dim=-1)\n",
    "        input_data2 = torch.cat ((gps_data2, grid_data2), dim=-1)\n",
    "        \n",
    "        _, (hidden1, _) = self.rnn_encoder (input_data1)\n",
    "        _, (hidden2, _) = self.rnn_encoder (input_data2)\n",
    "        \n",
    "        hidden = torch.cat((hidden1, hidden2, torch.mul (hidden1, hidden2), torch.abs (hidden1 - hidden2)), -1).squeeze()\n",
    "        hidden = self.transform(hidden).unsqueeze(2)\n",
    "        \n",
    "        return hidden.unsqueeze(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transform(\"2-layer-highway\", input_size=512, hidden_size=512, output_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.rand(3, 512)\n",
    "output = model(input_)\n",
    "rnn = RNNEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "in1 = torch.rand(3, 32, 2)\n",
    "in2 = torch.rand(3, 32, 2)\n",
    "on1 = torch.rand(3, 32, 128)\n",
    "on2 = torch.rand(3, 32, 128)\n",
    "out = rnn(in1, on1, in2, on2)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
